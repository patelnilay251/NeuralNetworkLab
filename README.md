# NeuralNetworkLab

This repository contains a Python implementation of a neural network with dropout and regularization techniques applied. The neural network is built from scratch using NumPy and includes classes for dropout, regularization, activation functions (ReLU, sigmoid, tanh), forward and backward propagation, as well as a model class.

# Features

1. Implementation of Neural Network: The repository provides a complete implementation of a neural network with support for dropout and regularization techniques.
2. Customization: The architecture allows for customization of parameters such as dropout rate, regularization type, and strength, enabling experimentation with different configurations.
3. Activation Functions: Various activation functions including ReLU, sigmoid, and tanh are supported, providing flexibility in designing the network architecture.
4. Training on Classification Tasks: The network can be trained on datasets for classification tasks, making it suitable for a wide range of applications.
5. Model Evaluation: The model's performance can be evaluated using accuracy metrics, enabling assessment of its effectiveness in classification tasks.

# Usage 

1. Clone the repository to your local machine:

   ```
   git clone https://github.com/your-username/neural-network-with-dropout-and-regularization.git

   ``` 
2. Install the required dependencies:

   ```
    pip install numpy scikit-learn

   ``` 
3. Run the provided example script to train and evaluate the model:

   ```
   python main.py

   ``` 

# Dataset

The example script utilizes the Breast Cancer dataset from the scikit-learn library. You can replace this dataset with your own data by modifying the code accordingly.


# Acknowledgements

This implementation is inspired by various resources and tutorials on neural networks, dropout, and regularization techniques.


