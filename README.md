# NeuralNetworkLab

This repository contains a Python implementation of a neural network with dropout and regularization techniques applied. The neural network is built from scratch using NumPy and includes classes for dropout, regularization, activation functions (ReLU, sigmoid, tanh), forward and backward propagation, as well as a model class.

# Features

1. Implementation of a neural network with dropout and regularization.
2. Flexible architecture allowing customization of parameters such as dropout rate, regularization type, and strength.
3. Support for multiple activation functions including ReLU, sigmoid, and tanh.
4. Ability to train the model on datasets for classification tasks.
5. Evaluation of model performance through accuracy metrics.

# Usage 

1. Clone the repository to your local machine:

   ```
   git clone https://github.com/your-username/neural-network-with-dropout-and-regularization.git

   ``` 
2. Install the required dependencies:

   ```
    pip install numpy scikit-learn

   ``` 
3. Run the provided example script to train and evaluate the model:

   ```
   python main.py

   ``` 

# Dataset

The example script utilizes the Breast Cancer dataset from the scikit-learn library. You can replace this dataset with your own data by modifying the code accordingly.


# Acknowledgements

This implementation is inspired by various resources and tutorials on neural networks, dropout, and regularization techniques.


